{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# did not figure out the dimension of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch\n",
    "import numpy as np\n",
    "affine_par = True\n",
    "\n",
    "\n",
    "def outS(i):\n",
    "    i = int(i)\n",
    "    i = (i+1)/2\n",
    "    i = int(np.ceil((i+1)/2.0))\n",
    "    i = (i+1)/2\n",
    "    return int(i)\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, affine = affine_par)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, affine = affine_par)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1,  dilation_ = 1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\n",
    "        self.bn1 = nn.BatchNorm2d(planes,affine = affine_par)\n",
    "        for i in self.bn1.parameters():\n",
    "            i.requires_grad = False\n",
    "        padding = 1\n",
    "        if dilation_ == 2:\n",
    "            padding = 2\n",
    "        elif dilation_ == 4:\n",
    "            padding = 4\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\n",
    "                               padding=padding, bias=False, dilation = dilation_)\n",
    "        self.bn2 = nn.BatchNorm2d(planes,affine = affine_par)\n",
    "        for i in self.bn2.parameters():\n",
    "            i.requires_grad = False\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4, affine = affine_par)\n",
    "        for i in self.bn3.parameters():\n",
    "            i.requires_grad = False\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Classifier_Module(nn.Module):\n",
    "\n",
    "    def __init__(self,dilation_series,padding_series,NoLabels):\n",
    "        super(Classifier_Module, self).__init__()\n",
    "        self.conv2d_list = nn.ModuleList()\n",
    "        for dilation,padding in zip(dilation_series,padding_series):\n",
    "            self.conv2d_list.append(nn.Conv2d(2048,NoLabels,kernel_size=3,stride=1, padding =padding, dilation = dilation,bias = True))\n",
    "\n",
    "        for m in self.conv2d_list:\n",
    "            m.weight.data.normal_(0, 0.01)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv2d_list[0](x)\n",
    "        for i in range(len(self.conv2d_list)-1):\n",
    "            out += self.conv2d_list[i+1](x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers,NoLabels):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64,affine = affine_par)\n",
    "        for i in self.bn1.parameters():\n",
    "            i.requires_grad = False\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation__ = 2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation__ = 4)\n",
    "        self.layer5 = self._make_pred_layer(Classifier_Module, [6,12,18,24],[6,12,18,24],NoLabels)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        #        for i in m.parameters():\n",
    "        #            i.requires_grad = False\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1,dilation__ = 1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion or dilation__ == 2 or dilation__ == 4:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion,affine = affine_par),\n",
    "            )\n",
    "        for i in downsample._modules['1'].parameters():\n",
    "            i.requires_grad = False\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride,dilation_=dilation__, downsample = downsample ))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes,dilation_=dilation__))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    def _make_pred_layer(self,block, dilation_series, padding_series,NoLabels):\n",
    "        return block(dilation_series,padding_series,NoLabels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MS_Deeplab(nn.Module):\n",
    "    def __init__(self,block,NoLabels):\n",
    "        super(MS_Deeplab,self).__init__()\n",
    "        self.Scale = ResNet(block,[3, 4, 23, 3],NoLabels)   #changed to fix #4 \n",
    "\n",
    "    def forward(self,x):\n",
    "        input_size = x.size()[2]\n",
    "        self.interp1 = nn.UpsamplingBilinear2d(size = (  int(input_size*0.75)+1,  int(input_size*0.75)+1  ))\n",
    "        self.interp2 = nn.UpsamplingBilinear2d(size = (  int(input_size*0.5)+1,   int(input_size*0.5)+1   ))\n",
    "        self.interp3 = nn.UpsamplingBilinear2d(size = (  outS(input_size),   outS(input_size)   ))\n",
    "        out = []\n",
    "        x2 = self.interp1(x)\n",
    "        x3 = self.interp2(x)\n",
    "        out.append(self.Scale(x))        # for original scale\n",
    "        out.append(self.interp3(self.Scale(x2)))        # for 0.75x scale\n",
    "        out.append(self.Scale(x3))        # for 0.5x scale\n",
    "\n",
    "\n",
    "        x2Out_interp = out[1]\n",
    "        x3Out_interp = self.interp3(out[2])\n",
    "        temp1 = torch.max(out[0],x2Out_interp)\n",
    "        out.append(torch.max(temp1,x3Out_interp))\n",
    "        return out\n",
    "\n",
    "def Res_Deeplab(NoLabels=21):\n",
    "    model = MS_Deeplab(Bottleneck,NoLabels)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Res_Deeplab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wxk/anaconda2/envs/py3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:183: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.Upsample instead.\n",
      "  warnings.warn(\"nn.UpsamplingBilinear2d is deprecated. Use nn.Upsample instead.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fe47afce562c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "input = Variable(torch.randn(1, 3, 224, 224))\n",
    "out = net(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-5.5457 -4.5190 -4.2877 -4.7061 -5.6281 -5.7389 -3.8775 -3.1976 -6.8529 -4.5621\n",
      "-4.3067 -4.1151 -3.7508 -2.8986 -1.2433 -5.6969 -4.9418 -3.4316 -5.6195 -4.3666\n",
      "-4.8359 -4.2008 -3.4180 -2.7728 -2.5507 -5.6849 -5.1134 -3.5116 -3.5550 -4.2708\n",
      "-5.5632 -4.2247 -3.2130 -3.0625 -4.3078 -5.1033 -5.3000 -4.7370 -3.2537 -4.7453\n",
      "-4.9185 -3.6355 -3.0595 -2.5015 -1.2722 -3.3529 -6.4096 -8.4070 -7.3098 -6.2609\n",
      "-2.8117 -3.0207 -3.0898 -3.0206 -2.8142 -2.9641 -3.2649 -4.1018 -5.8599 -4.2756\n",
      "-2.0412 -2.9003 -2.7998 -2.7365 -3.7072 -2.0094 -1.6338 -2.1928 -3.2992 -3.7538\n",
      "-2.5431 -3.3069 -2.5463 -2.0445 -3.5848 -1.0435 -1.0356 -1.5277 -0.4867 -3.0435\n",
      "-4.2534 -4.2732 -2.6864 -1.3400 -2.0806 -0.6208 -0.9896 -0.9542  1.7182 -0.4922\n",
      "-2.6738 -4.0940 -2.4845 -1.0232 -2.8879 -1.2943 -0.8629 -0.7197  0.0094 -1.0676\n",
      "-1.6231 -2.7019 -2.7168 -2.5475 -3.0738 -1.6063 -1.1543 -1.2400 -1.3854 -2.0141\n",
      "-1.3966 -1.6986 -2.9910 -3.7345 -2.3897 -1.8241 -1.3306 -1.2658 -1.9865 -2.1626\n",
      "-2.2895 -2.6857 -2.9147 -2.4054 -0.5867 -2.2151 -0.8584  0.4524 -1.3140 -0.3440\n",
      "-2.1130 -1.7043 -1.1850 -1.2413 -2.5592 -0.0588 -0.9418 -1.9217  0.2881  0.7138\n",
      "-1.7164 -1.0143 -1.2358 -1.9750 -2.8263  0.5430 -0.2342 -1.6938 -0.3712  0.9450\n",
      "-0.7283 -0.4863 -1.7605 -2.7033 -1.4668  0.4756  0.3998 -0.5285 -1.1435  0.5538\n",
      " 1.2224  0.0094 -1.4527 -1.5227  1.4408  0.6243  0.0955 -0.0908  0.1199 -0.2556\n",
      " 0.4662 -0.5780 -3.0508 -4.2800 -1.5935 -0.2765 -0.5189 -1.0081 -0.4312 -2.5391\n",
      " 0.3576  0.2730 -1.8342 -3.6670 -2.9283  0.1046  0.5921  0.2282  0.7065 -1.4073\n",
      "-0.0127  0.8185 -0.6612 -2.5605 -2.9882  0.1779  1.2006  1.3701  1.9764 -0.0991\n",
      "-1.5538 -0.6854 -2.3900 -3.8375 -2.1980 -1.6458 -0.9214  0.1698  1.8223 -1.8540\n",
      "-1.7877 -0.8769 -0.5297 -0.4086 -0.1759 -0.4490 -1.4240 -1.9842 -1.0127 -0.4708\n",
      "-1.4937 -1.2003 -0.5821 -0.1500 -0.4151  1.1430 -0.1031 -1.9889 -2.3505 -0.2296\n",
      "-1.1911 -1.7850 -1.2945 -0.7550 -1.2017  1.7223  0.6341 -1.7515 -2.7195 -1.0795\n",
      "-1.3993 -2.7602 -1.4144  0.0833 -0.8218 -0.1186 -1.6201 -3.1791 -2.6481 -2.9695\n",
      "-3.8782 -2.6996 -1.6967 -0.7714  0.1746  0.2230 -1.7250 -2.9069 -0.5603 -2.0630\n",
      "-4.1058 -3.2805 -2.9864 -2.6452 -1.6788 -0.5795 -0.9856 -1.7833 -1.8591 -0.8895\n",
      "-2.4334 -3.5415 -4.0257 -3.9386 -3.3330 -1.3549 -0.2232 -0.6879 -3.4992  0.0869\n",
      " 0.7875 -2.5212 -3.5570 -3.0521 -1.7390 -0.9318 -0.2593 -0.5007 -2.4352  0.4016\n",
      "\n",
      "Columns 10 to 19 \n",
      "-4.3876 -4.1041 -1.4867 -1.5303 -2.5728 -4.1403 -5.7592 -4.0106 -2.7520 -2.2642\n",
      "-4.8278 -5.8472 -6.2692 -5.3776 -3.4653 -2.1766 -3.1558 -3.5961 -3.6258 -3.0574\n",
      "-4.5667 -4.8057 -5.3506 -5.0559 -3.3821 -2.0906 -2.9429 -2.3690 -3.1930 -3.5807\n",
      "-4.2211 -3.0037 -2.4159 -2.8698 -2.4897 -2.0777 -2.4353 -1.5484 -2.4948 -3.2513\n",
      "-4.4074 -2.4652 -1.1501 -1.1234 -0.9545 -0.3329  1.0521 -2.3538 -2.5723 -1.4864\n",
      "-2.5713 -1.7128 -2.6659 -1.6312 -2.8789 -3.2895  0.2563 -2.4515 -2.6638 -1.5500\n",
      "-2.8140 -2.0509 -3.0359 -1.3791 -2.5089 -3.1062  0.1477 -0.7450 -1.8366 -2.1472\n",
      "-3.2017 -2.5812 -2.8016 -1.4738 -1.5103 -1.3859  0.4242  0.6846 -0.6605 -1.9005\n",
      "-1.8007 -2.4054 -2.5045 -3.0221 -1.5489  0.2686  0.7839 -0.2440  0.2948  0.5671\n",
      "-0.7611 -0.4503 -1.5145 -3.7091 -3.1841 -1.6169 -0.6853 -2.3639 -1.6619 -0.7682\n",
      "-2.0896 -1.6643 -0.7907 -1.4943 -1.2114 -0.8708 -1.4013 -2.7855 -2.8311 -2.2767\n",
      "-3.2059 -3.2235 -0.3221  0.0883  0.0136 -0.5106 -1.4483 -1.7564 -2.5806 -2.9629\n",
      "-1.5299 -2.3038 -0.0979 -2.4957 -3.8651 -3.5540 -0.9103  0.4758 -0.2782 -1.8314\n",
      " 0.7455  0.8355  1.4355  0.7544 -0.8310 -2.0202 -1.5128  2.4912  1.9482 -0.1791\n",
      " 1.5591  1.4139  0.4522 -0.5881 -0.0096  0.6387 -0.1922  2.7264  1.5044 -0.7060\n",
      " 1.2278  0.7867 -0.8609 -1.8614  0.1472  1.8722  0.0207  1.1902  0.1502 -1.1102\n",
      " 0.0685  0.3091 -0.3166  1.5966  1.1877 -0.8702 -3.9043 -2.1090 -0.3545  0.9101\n",
      "-2.2128 -0.3383  2.1984  3.1737  0.8258 -1.7425 -1.4286 -2.6103 -1.5334 -0.3443\n",
      "-1.7567 -0.9423  0.4353  0.9402 -1.1046 -2.6047 -0.4654 -1.4184 -0.9697 -0.8547\n",
      "-1.0348 -1.4749 -2.0639 -0.8070 -2.0548 -2.9049 -0.4548  0.2124 -0.4037 -1.5551\n",
      "-2.5190 -1.9082 -1.7567  2.2285  0.5237 -2.0913 -0.8370  1.0276 -1.5757 -3.3792\n",
      "-0.3004  0.0279  1.0436  2.0664  0.6673 -1.0775 -1.0920 -1.1748 -2.0169 -1.9780\n",
      "-0.2814 -0.7393  0.1633  0.6932 -0.2848 -1.3275 -0.9918 -3.0333 -2.9760 -1.8898\n",
      "-1.2240 -1.9242 -1.9512 -1.1353 -1.2949 -1.6220 -1.3089 -3.1591 -3.1984 -2.3277\n",
      "-1.8902 -1.2412 -2.8538 -2.6634 -1.3252 -0.7417 -2.8158 -0.1635 -1.4298 -2.5046\n",
      "-0.8964 -0.1796 -3.0319 -1.7705 -0.1073  0.8181 -0.1342 -1.3576 -1.4571 -1.1394\n",
      "-0.0465 -0.4939 -3.3952 -1.1995  0.1985  0.3306 -1.2719 -0.8352 -0.5704 -0.6106\n",
      " 0.2986 -1.2143 -2.8024 -1.2108 -0.7039 -1.2796 -2.9354 -0.2040 -0.1843 -0.8960\n",
      "-0.2220 -1.3716 -0.1124 -2.0645 -3.1109 -3.0877 -1.8311 -1.0715 -1.7135 -1.9737\n",
      "\n",
      "Columns 20 to 28 \n",
      "-2.8279 -1.5898 -2.0874 -2.7246 -1.9052 -0.7763 -1.5094 -2.6066 -2.5701\n",
      "-1.7035 -2.4323 -2.3310 -2.1654 -2.7010 -3.7883 -3.5748 -3.3469 -4.3916\n",
      "-1.6977 -2.8521 -2.3366 -1.6517 -2.2982 -4.0860 -4.2686 -3.8197 -3.7127\n",
      "-1.7948 -2.9550 -2.6728 -2.0464 -2.1742 -3.3893 -4.0360 -3.9016 -2.7731\n",
      "-0.9791 -2.8468 -3.9085 -4.2122 -3.8060 -3.4183 -3.3220 -3.4693 -3.8124\n",
      "-0.2800 -1.7041 -2.1113 -1.8714 -1.3541 -1.1944 -0.9866 -1.3437 -2.8788\n",
      "-0.6965 -2.0669 -2.1892 -1.5791 -0.7521 -0.0124 -0.4116 -1.5925 -3.1976\n",
      "-1.3247 -2.4390 -2.6957 -2.3781 -1.7696 -0.7860 -1.3502 -2.5970 -3.6614\n",
      "-1.2604 -1.3245 -2.1843 -3.3110 -4.1758 -4.4291 -3.5552 -2.7382 -3.1627\n",
      "-1.8718 -2.5107 -3.0858 -2.3803  0.8223 -1.4496 -1.4559 -1.0249 -1.9850\n",
      "-1.8607 -2.8695 -2.1268 -0.6403  0.5822 -1.3016 -0.7709 -0.1555 -1.7851\n",
      "-1.9451 -3.1073 -1.2932  0.1744 -2.0273 -1.8972 -0.8023 -0.3273 -2.0569\n",
      "-2.8431 -3.9308 -2.5711 -1.6709 -4.1375 -1.1490 -0.8526 -1.7379 -2.2944\n",
      "-0.9279 -0.6585 -1.0793 -0.7183  1.8965  1.0427  0.3147 -0.7284 -2.5271\n",
      "-0.7521  0.1009 -0.6994 -1.1261  0.8473  0.8775  0.1405 -0.6774 -0.8893\n",
      "-0.6021 -0.4876 -0.7195 -1.2761 -2.1356 -0.2387 -0.3983 -0.8338  0.2354\n",
      " 1.2355 -1.2589 -0.4277  0.4505 -1.9028 -0.9002 -0.3246 -0.4465 -1.5364\n",
      "-1.1892 -0.9971  0.2365  1.0830  0.1137 -1.1016 -2.1857 -2.3352 -0.7466\n",
      "-2.8086 -0.6338  0.4062  0.7596  0.8744 -0.3683 -1.5711 -1.8760 -0.4255\n",
      "-2.4937  0.1448  0.5863  0.0859 -0.1014  0.0451 -0.1387 -0.4250 -0.5859\n",
      " 0.8845  1.6526  1.2818 -0.3324 -3.2945 -1.1158  0.4534  0.6621 -1.2408\n",
      " 0.5826  0.4347 -0.9526 -2.2751 -2.2284 -1.6373 -1.5156 -1.5302 -1.3483\n",
      "-0.8448 -0.4979 -1.1367 -1.7537 -1.3410 -0.7536 -1.3604 -2.2563 -2.5362\n",
      "-1.4480 -0.7080 -0.5000 -0.7326 -1.3139 -0.9196 -1.3455 -2.0761 -2.5961\n",
      " 0.7224  0.2414 -0.2720 -1.1761 -2.8288 -4.5906 -3.7355 -1.5498  0.6801\n",
      "-1.1111  0.6421  0.4237 -0.6321 -1.3913 -2.8221 -2.5523 -1.3428  0.0454\n",
      "-1.0887  0.1080 -0.2570 -0.8172 -0.2060 -1.6483 -2.3464 -2.0424 -0.4786\n",
      "-0.3586 -0.7560 -1.0537 -0.6825  0.9268 -0.8505 -2.0917 -2.3835 -1.3128\n",
      "-0.0687 -1.3451 -0.7060  0.8206  2.2065 -0.2097 -0.7621 -1.1013 -2.8783\n",
      "[torch.FloatTensor of size 29x29]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out[1][0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
