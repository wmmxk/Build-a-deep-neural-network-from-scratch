{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Motivation:\n",
    "          Pass the cell_state of the last cell in the previous episode to the next episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Aspiration source: https://medium.com/@erikhallstrm/using-the-tensorflow-lstm-api-3-7-5f2b97ca6b73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The way in the previous 1_Stateless_LSTM does not work. You need to feed in throught a placehoder\n",
    "\n",
    "1. define the placehoder\n",
    "cell_state_t = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "hidden_state_t = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "init_state_t = tf.nn.rnn_cell.LSTMStateTuple(cell_state_t, hidden_state_t)\n",
    "\n",
    "2. use the init_state (placehoder) as an arguemnt of tf.nn.rnn\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(state_size, state_is_tuple=True)\n",
    "states_series, current_state = tf.nn.rnn(cell, inputs_series, init_state_t)\n",
    "\n",
    "3. The initial state vaules which will feed into the placehoder\n",
    "_current_cell_state = np.zeros((batch_size, state_size))\n",
    "_current_hidden_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "4. Run an epoch of training and store the cell_state of the last cell\n",
    "_total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "    [total_loss, train_step, current_state, predictions_series],\n",
    "    feed_dict={\n",
    "        batchX_placeholder: batchX,\n",
    "        batchY_placeholder: batchY,\n",
    "        cell_state_t: _current_cell_state,\n",
    "        hidden_state_t: _current_hidden_state\n",
    "\n",
    "    })\n",
    "\n",
    "5. update the initial state value. So the next time when you run step 4, the initial state of the first cell\n",
    "   in the episode will not be zero\n",
    "_current_cell_state, _current_hidden_state = _current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Trap:\n",
    "    The two placehoders don't have to be in the class body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To do\n",
    "    If you also want to feed the hidden state to the next episode, you need to cancatenate the hidden_state\n",
    "    with the input manually. \n",
    "    \n",
    "    The hidden_state is merged with input automatically between different cells within an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "state_size = 4\n",
    "input_len =5\n",
    "batch_size=3\n",
    "num_step = 20\n",
    "\n",
    "np.random.seed(0)\n",
    "input = np.random.rand(batch_size,input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.rnn_cell = None\n",
    "        self.initialized = False\n",
    "        self.batch_size = None\n",
    "        self.output = None\n",
    "        self.rnn_state_t=None\n",
    "    def first(self,input_t):\n",
    "        self.batch_size = tf.unpack(tf.shape(input_t))[0]\n",
    "        with tf.variable_scope(\"rnn\"):\n",
    "            self.rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(state_size,state_is_tuple=True)\n",
    "            self.rnn_state_t = self.rnn_cell.zero_state(self.batch_size,tf.float32)\n",
    "        output_= self.predict(input_t)\n",
    "        self.initialized = True\n",
    "        return output_\n",
    "    \n",
    "    def predict(self,input_t):\n",
    "\n",
    "        if self.initialized:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        with tf.variable_scope(\"rnn/RNN\"):\n",
    "\n",
    "            output_, current_state_t = self.rnn_cell(input_t, init_state_t)\n",
    "\n",
    "        return output_ ,current_state_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input placehoders\n",
    "input_t = tf.placeholder(tf.float32,(None,input_len))\n",
    "\n",
    "\n",
    "cell_state_t = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "hidden_state_t = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "init_state_t = tf.nn.rnn_cell.LSTMStateTuple(cell_state_t, hidden_state_t)\n",
    "\n",
    "# define operations\n",
    "agent = Agent()\n",
    "output1_t = agent.first(input_t)\n",
    "output2_t = agent.predict(input_t)\n",
    "\n",
    "# initialization operation\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "_current_cell_state = np.zeros((batch_size, state_size))\n",
    "_current_hidden_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "output1_v1,output2_v1 = sess.run([output1_t, output2_t],{input_t: input, cell_state_t: _current_cell_state,\n",
    "        hidden_state_t: _current_hidden_state})\n",
    "\n",
    "_current_cell_state, _current_hidden_state = output2_v1[1]\n",
    "\n",
    "save_path = saver.save(sess, './rnn_checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output1_v2,output2_v2 = sess.run([output1_t, output2_t],{input_t: input, cell_state_t: _current_cell_state,\n",
    "        hidden_state_t: _current_hidden_state})\n",
    "\n",
    "_current_cell_state, _current_hidden_state = output2_v1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output1_v3,output2_v3 = sess.run([output1_t, output2_t],{input_t: input, cell_state_t: _current_cell_state,\n",
    "        hidden_state_t: _current_hidden_state})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shows the initial state changes\n",
    "(output2_v1[0]==output2_v2[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This shows if you use the initial cell state, the output is the same\n",
    "(output2_v2[0] == output2_v3[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"./states_saved/cell_state.npy\",_current_cell_state)\n",
    "np.save(\"./states_saved/hidden_state.npy\",_current_hidden_state)\n",
    "\n",
    "np.save(\"./states_saved/output2_v.npy\",output2_v3[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
